{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "075338c2-b007-40b5-ab95-efaa2a983c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for scrapping welt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b52892-0a9c-4a60-92e9-453076a2c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "CLIMATE_KEYWORDS = [\"klima\",  \"klimakrise\", \"klimawandel\", \"erderwärmung\", \"globale erwärmung\", \"treibhauseffekt\", \"treibhausgas\",\n",
    "    \"co2\", \"kohlendioxid\", \"emission\", \"emissionen\", \"energiewende\", \"erneuerbare energien\", \"klimaschutz\", \"klimapolitik\",\n",
    "    \"hitzewelle\", \"dürre\", \"hochwasser\", \"wasserknappheit\", \"starkregen\", \"waldbrand\",\n",
    "    \"gletscherschmelze\", \"artensterben\", \"klimaneutral\", \"emissionshandel\"]\n",
    "STRICT_KEYWORDS = [\"klima\", \"klimakrise\", \"klimawandel\", \"erderwärmung\", \"co2\", \"kohlendioxid\", \"emission\", \"emissionen\",\n",
    "    \"energiewende\", \"klimaschutz\", \"klimaneutral\", \"treibhauseffekt\", \"treibhausgas\", \"hitzewelle\", \"dürre\", \"hochwasser\",\n",
    "    \"wasserknappheit\", \"starkregen\", \"waldbrand\"]\n",
    "CLASSIFICATION_KEYWORDS = {\n",
    "    \"Climate_Policy\": [\"gesetz\", \"politik\", \"regierung\", \"beschluss\", \"verordnung\", \"ziel\", \"klimaziel\", \"bundestag\", \"eu\", \"parlament\", \"ministerium\"],\n",
    "    \"Climate_Science\": [\"studie\", \"forschung\", \"wissenschaft\", \"ipcc\", \"daten\", \"analyse\", \"bericht\", \"modell\", \"forscher\"],\n",
    "    \"Energy_Transition\": [\"energiewende\", \"erneuerbar\", \"solar\", \"windkraft\", \"kohlekraft\", \"atomkraft\", \"wasserstoff\", \"stromnetz\"],\n",
    "    \"Climate_Economy\": [\"kosten\", \"industrie\", \"wirtschaft\", \"markt\", \"unternehmen\", \"investition\", \"preis\", \"arbeitsplätze\"],\n",
    "    \"Climate_Activism\": [\"protest\", \"demonstration\", \"aktivisten\", \"fridays for future\", \"ngo\", \"bewegung\"],\n",
    "    \"Climate_Impact\": [\"hitzewelle\", \"dürre\", \"hochwasser\", \"überschwemmung\", \"starkregen\", \"waldbrand\", \"extremwetter\"],\n",
    "    \"Climate_Geopolitics\": [\"china\", \"usa\", \"eu\", \"russland\", \"international\", \"global\", \"weltweit\", \"g7\", \"g20\"],\n",
    "    \"Climate_Opinion\": [\"meinung\", \"kommentar\", \"kolumne\", \"leitartikel\", \"debatte\", \"gastbeitrag\"]}\n",
    "TIME_WINDOW = \"y4\"\n",
    "OFFSET_STEP = 10\n",
    "OUTPUT_CSV = \"scrapped_welt.csv\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Referer\": \"https://www.welt.de/suche\",}\n",
    "SOURCE = \"WELT\"\n",
    "LANGUAGE = \"de\"\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def strict_relevance(title, content):\n",
    "    title = title.lower()\n",
    "    content_l = content.lower()\n",
    "    lead = \" \".join(content_l.split()[:300])\n",
    "    title_hit = any(k in title for k in STRICT_KEYWORDS)\n",
    "    lead_hits = sum(k in lead for k in STRICT_KEYWORDS)\n",
    "    return title_hit or lead_hits >= 3\n",
    "def extract_jsonld(soup):\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "        except:\n",
    "            continue\n",
    "        if isinstance(data, dict) and data.get(\"@type\") == \"NewsArticle\":\n",
    "            content = clean_text(data.get(\"articleBody\"))\n",
    "            headline = clean_text(data.get(\"headline\"))\n",
    "            author = None\n",
    "            auth = data.get(\"author\")\n",
    "            if isinstance(auth, dict):\n",
    "                author = auth.get(\"name\")\n",
    "            elif isinstance(auth, list) and auth:\n",
    "                author = auth[0].get(\"name\")\n",
    "            return content, headline, clean_text(author)\n",
    "    return None, None, None\n",
    "def extract_fallback_content(soup):\n",
    "    texts = []\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        t = p.get_text().strip()\n",
    "        if len(t) < 80:\n",
    "            continue\n",
    "        if any(x in t.lower() for x in [\"anzeige\", \"newsletter\", \"datenschutz\", \"jetzt abonnieren\", \"quelle:\", \"bild:\"]):\n",
    "            continue\n",
    "        texts.append(t)\n",
    "    content = clean_text(\" \".join(texts))\n",
    "    return content if content and len(content) > 800 else None\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    actors = sorted(set(ent.text for ent in doc.ents if ent.label_ in [\"PER\", \"ORG\"]))\n",
    "    sentences = list(doc.sents)\n",
    "    return actors, len(actors), len(sentences), len(text)\n",
    "def classify_article(title, intro, content):\n",
    "    text = f\"{title} {intro} {' '.join(content.split()[:300])}\".lower()\n",
    "    scores = {}\n",
    "    for cat, keywords in CLASSIFICATION_KEYWORDS.items():\n",
    "        scores[cat] = sum(k in text for k in keywords)\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other_Climate\"\n",
    "rows = []\n",
    "seen_urls = set()\n",
    "for keyword in CLIMATE_KEYWORDS:\n",
    "    print(f\"\\nKeyword: {keyword}\")\n",
    "    offset = 0\n",
    "    while True:\n",
    "        api_url = f\"https://www.welt.de/api/search/{keyword}\"\n",
    "        params = {\"offset\": offset, \"restrictBy\": TIME_WINDOW}\n",
    "        try:\n",
    "            r = requests.get(api_url, params=params, headers=HEADERS, timeout=15)\n",
    "            data = r.json()\n",
    "        except:\n",
    "            break\n",
    "        items = data.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        for item in items:\n",
    "            url = item.get(\"url\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            html = requests.get(url, headers=HEADERS, timeout=15).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            content, headline, author = extract_jsonld(soup)\n",
    "            if not content:\n",
    "                content = extract_fallback_content(soup)\n",
    "            if not headline:\n",
    "                h1 = soup.find(\"h1\")\n",
    "                headline = clean_text(h1.get_text()) if h1 else None\n",
    "            if not content or not headline:\n",
    "                continue\n",
    "            if not strict_relevance(headline, content):\n",
    "                continue\n",
    "            actors, actor_count, sent_count, length = analyze_text(content)\n",
    "            article_class = classify_article(headline, item.get(\"intro\"), content)\n",
    "            rows.append({\n",
    "                \"URL\": url,\n",
    "                \"Source\": SOURCE,\n",
    "                \"Language\": LANGUAGE,\n",
    "                \"Published_Date\": item.get(\"publicationDate\"),\n",
    "                \"Keyword_Matched\": keyword,\n",
    "                \"Article_Classification\": article_class,\n",
    "                \"Headline\": headline,\n",
    "                \"Intro\": clean_text(item.get(\"intro\")),\n",
    "                \"Content\": content,\n",
    "                \"Content_Length\": length,\n",
    "                \"Sentence_Count\": sent_count,\n",
    "                \"Actors\": \", \".join(actors),\n",
    "                \"Actor_Count\": actor_count,\n",
    "                \"Author\": author\n",
    "            })\n",
    "            print(\"Saved\")\n",
    "            time.sleep(1)\n",
    "        offset += OFFSET_STEP\n",
    "        time.sleep(0.5)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"File saved: {OUTPUT_CSV}\")\n",
    "print(f\"Total number of articles scrapped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df78bf3-7fc6-4fb8-a303-2b1c5542d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for scrapping FAZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f0979-ad44-4275-a2f9-cd5f8a2b07d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "INVALID_URL_PATTERNS = [\"/video/\", \"/podcast/\", \"/audio/\", \"/bilder/\", \"/einspruch/\", \"/interaktiv/\", \"/explainer/\",\n",
    "    \"fazarchiv.faz.net\", \"/payment/\", \"/abo/\"]\n",
    "MAX_RETRIES_PER_URL = 2\n",
    "CLIMATE_KEYWORDS_DE = [\"klima\", \"klimawandel\", \"klimakrise\", \"erderwärmung\", \"co2\", \"emission\", \"energiewende\", \"hochwasser\",\n",
    "    \"dürre\", \"hitzewelle\", \"klimaschutz\"]\n",
    "CLASSIFICATION_KEYWORDS = {\n",
    "    \"Climate_Policy\": [\"gesetz\", \"politik\", \"regierung\", \"beschluss\", \"verordnung\", \"klimaziel\", \"bundestag\",\"parlament\", \"ministerium\", \"regulierung\"],\n",
    "    \"Climate_Science\": [\"studie\", \"forschung\", \"wissenschaft\", \"ipcc\", \"daten\", \"analyse\", \"bericht\", \"modell\", \"forscher\"],\n",
    "    \"Energy_Transition\": [\"energiewende\", \"erneuerbar\", \"solar\", \"windkraft\", \"kohlekraft\", \"atomkraft\", \"wasserstoff\", \"stromnetz\"],\n",
    "    \"Climate_Economy\": [\"kosten\", \"industrie\", \"wirtschaft\", \"markt\", \"unternehmen\", \"investition\", \"preis\", \"arbeitsplätze\"],\n",
    "    \"Climate_Activism\": [\"protest\", \"demonstration\", \"aktivisten\", \"fridays for future\", \"ngo\", \"bewegung\"],\n",
    "    \"Climate_Impact\": [\"hitzewelle\", \"dürre\", \"hochwasser\", \"überschwemmung\", \"starkregen\", \"waldbrand\"],\n",
    "    \"Climate_Geopolitics\": [\"china\", \"usa\", \"russland\", \"international\", \"global\", \"weltweit\", \"g7\", \"g20\", \"eu\"],\n",
    "    \"Climate_Opinion\": [\"meinung\", \"kommentar\", \"kolumne\", \"leitartikel\", \"debatte\", \"gastbeitrag\"]}\n",
    "FAZ_API = \"https://www.faz.net/api/faz-content-search\"\n",
    "BASE_KEYWORDS = [\"klima\", \"klimawandel\", \"klimakrise\", \"co2\"]\n",
    "MAX_PAGES = 5\n",
    "MIN_YEAR = 2022\n",
    "OUTPUT_FILE = \"scrapped_faz.csv\"\n",
    "API_HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/json\",\n",
    "    \"Referer\": \"https://www.faz.net/suche/\"}\n",
    "def extract_actors(text):\n",
    "    doc = nlp(text[:4000])\n",
    "    return sorted(set(ent.text for ent in doc.ents if ent.label_ in (\"PER\", \"ORG\")))\n",
    "def sentence_count(text):\n",
    "    return sum(1 for _ in nlp(text).sents)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")\n",
    "chrome_options.add_argument(\"--disable-notifications\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0\")\n",
    "chrome_options.add_argument(r\"--user-data-dir=C:/selenium_chrome_profile\")\n",
    "driver = None\n",
    "wait = None\n",
    "def start_driver():\n",
    "    global driver, wait\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 25)\n",
    "def restart_driver():\n",
    "    global driver\n",
    "    try:\n",
    "        driver.quit()\n",
    "    except:\n",
    "        pass\n",
    "    start_driver()\n",
    "start_driver()\n",
    "def human_sleep(a=2, b=4):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "def safe_get(url, timeout=20):\n",
    "    try:\n",
    "        driver.set_page_load_timeout(timeout)\n",
    "        driver.get(url)\n",
    "        return True\n",
    "    except TimeoutException:\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        return False\n",
    "def is_valid_year(api_doc):\n",
    "    try:\n",
    "        return int(api_doc[\"date\"][:4]) >= MIN_YEAR\n",
    "    except:\n",
    "        return False\n",
    "def is_climate_article(title, content):\n",
    "    text = f\"{title} {content}\".lower()\n",
    "    hits = sum(text.count(k) for k in CLIMATE_KEYWORDS_DE)\n",
    "    return hits >= 3 or any(k in title.lower() for k in CLIMATE_KEYWORDS_DE)\n",
    "def classify_article(title, content):\n",
    "    text = f\"{title} {content}\".lower()\n",
    "    scores = {k: sum(text.count(w) for w in v)\n",
    "              for k, v in CLASSIFICATION_KEYWORDS.items()}\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other_Climate\"\n",
    "def extract_article():\n",
    "    paras = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "    clean = []\n",
    "    for p in paras:\n",
    "        t = p.text.strip()\n",
    "        if len(t) < 60:\n",
    "            continue\n",
    "        if any(x in t.lower() for x in [\"anzeige\", \"abo\", \"newsletter\"]):\n",
    "            continue\n",
    "        clean.append(t)\n",
    "    if len(clean) < 5:\n",
    "        return None, None\n",
    "    return clean[0], \" \".join(clean)\n",
    "def extract_author(api_doc):\n",
    "    authors = api_doc.get(\"metis_authors_full_names\")\n",
    "    if isinstance(authors, list):\n",
    "        return \", \".join(authors)\n",
    "    if isinstance(authors, str):\n",
    "        return authors\n",
    "    return None\n",
    "def extract_date():\n",
    "    try:\n",
    "        return driver.find_element(By.TAG_NAME, \"time\").text.strip()\n",
    "    except:\n",
    "        return None\n",
    "url_to_meta = {}\n",
    "for kw in BASE_KEYWORDS:\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        params = {\n",
    "            \"q\": kw,\n",
    "            \"page\": page,\n",
    "            \"rows\": 20,\n",
    "            \"paid_content\": \"include\",\n",
    "            \"sort_by\": \"date\",\n",
    "            \"sort_order\": \"desc\"}\n",
    "        r = requests.get(FAZ_API, headers=API_HEADERS, params=params, timeout=20)\n",
    "        docs = r.json().get(\"docs\", [])\n",
    "        for d in docs:\n",
    "            if not is_valid_year(d):\n",
    "                continue\n",
    "            url = d.get(\"url\")\n",
    "            if not url or any(bad in url for bad in INVALID_URL_PATTERNS):\n",
    "                continue\n",
    "            url_to_meta.setdefault(url, {\"keyword\": kw, \"api_doc\": d})\n",
    "        human_sleep(1, 2)\n",
    "urls = list(url_to_meta.keys())\n",
    "print(f\"URLs collected: {len(urls)}\")\n",
    "rows = []\n",
    "failed_urls = {}\n",
    "print(\"Scraping FAZ articles!\")\n",
    "for idx, url in enumerate(urls, 1):\n",
    "    meta = url_to_meta[url]\n",
    "    try:\n",
    "        if not safe_get(url):\n",
    "            continue\n",
    "        if \"fazarchiv.faz.net\" in driver.current_url:\n",
    "            print(\"Archive redirect detected, skipping:\", url)\n",
    "            continue\n",
    "        human_sleep(2, 4)\n",
    "        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"main\")))\n",
    "        headline = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "        intro, content = extract_article()\n",
    "        if not content or not is_climate_article(headline, content):\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"URL\": url,\n",
    "            \"Source\": \"FAZ\",\n",
    "            \"Language\": \"de\",\n",
    "            \"Published_Date\": extract_date(),\n",
    "            \"Keyword_Matched\": meta[\"keyword\"],\n",
    "            \"Article_Classification\": classify_article(headline, content),\n",
    "            \"Headline\": headline,\n",
    "            \"Intro\": intro,\n",
    "            \"Content\": content,\n",
    "            \"Content_Length\": len(content),\n",
    "            \"Sentence_Count\": sentence_count(content),\n",
    "            \"Actors\": \", \".join(extract_actors(content)),\n",
    "            \"Actor_Count\": len(extract_actors(content)),\n",
    "            \"Author\": extract_author(meta[\"api_doc\"])\n",
    "        })\n",
    "        print(f\"[{idx}/{len(urls)}] Saved:\", headline[:60])\n",
    "        if idx % 15 == 0:\n",
    "            print(\"Normal cool down to avoid bot detection\")\n",
    "            human_sleep(10, 15)\n",
    "    except Exception as e:\n",
    "        failed_urls[url] = failed_urls.get(url, 0) + 1\n",
    "        if failed_urls[url] >= MAX_RETRIES_PER_URL:\n",
    "            print(\"Permanently skipping:\", url)\n",
    "            continue\n",
    "        print(\"page was not able to load. Restarting the chromium driver.\")\n",
    "        restart_driver()\n",
    "        human_sleep(5, 8)\n",
    "        continue\n",
    "columns = [\"URL\", \"Source\", \"Language\", \"Published_Date\", \"Is_Premium\",\n",
    "    \"Keyword_Matched\", \"Article_Classification\", \"Headline\", \"Intro\", \"Content\", \"Content_Length\", \"Sentence_Count\", \"Actors\", \"Actor_Count\", \"Author\"]\n",
    "df = pd.DataFrame(rows)[columns]\n",
    "df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "driver.quit()\n",
    "print(f\"File saved: {OUTPUT_FILE}\")\n",
    "print(f\"Total number of Articles collected: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dd62d5-17b9-49ce-802a-0197b6a4bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COde for spiegel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad685c4-9b20-4f49-8aeb-ddcae0eff5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "CLIMATE_KEYWORDS = [\"klima\", \"klimakrise\", \"klimawandel\", \"erderwärmung\", \"globale erwärmung\", \"treibhauseffekt\", \"treibhausgas\",\n",
    "    \"co2\", \"kohlendioxid\", \"emission\", \"emissionen\", \"energiewende\", \"erneuerbare energien\", \"klimaschutz\", \"klimapolitik\",\n",
    "    \"hitzewelle\", \"dürre\", \"hochwasser\", \"wasserknappheit\", \"starkregen\", \"waldbrand\", \"gletscherschmelze\", \"artensterben\",\n",
    "    \"klimaneutral\", \"emissionshandel\"]\n",
    "SEARCH_API = \"https://www.spiegel.de/services/sitesearch/search\"\n",
    "SEGMENTS = \",\".join([\"spon\", \"spon_paid\", \"spon_international\", \"mmo\", \"mmo_paid\", \"hbm\", \"hbm_paid\", \"elf\", \"elf_paid\", \"effi\", \"effi_paid\"])\n",
    "MAX_PAGES = 10 \n",
    "PAGE_SIZE = 20\n",
    "OUTPUT_CSV = \"scrapped_spiegel.csv\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0\",\n",
    "    \"Accept\": \"application/json\"}\n",
    "SOURCE = \"SPIEGEL\"\n",
    "LANGUAGE = \"de\"\n",
    "def clean_text(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def keyword_hits(text):\n",
    "    text = str(text).lower()\n",
    "    return sum(k in text for k in CLIMATE_KEYWORDS)\n",
    "def extract_author(soup):\n",
    "    tag = soup.select_one('a[href*=\"/impressum/autor\"]')\n",
    "    return tag.get_text(strip=True) if tag else None\n",
    "def extract_published_date(soup):\n",
    "    tag = soup.select_one(\"time.timeformat\")\n",
    "    return tag.get(\"datetime\") if tag else None\n",
    "def is_boilerplate(text):\n",
    "    if not text:\n",
    "        return True\n",
    "    bad = [\"print-abo\", \"spiegel+\", \"digital-zugang\", \"jetzt weiterlesen\", \"abo abschließen\"]\n",
    "    return any(b in text.lower() for b in bad)\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    actors = sorted(set(ent.text for ent in doc.ents if ent.label_ in [\"PER\", \"ORG\"]))\n",
    "    sentences = list(doc.sents)\n",
    "    return actors, len(actors), len(sentences), len(text)\n",
    "def classify_article(text):\n",
    "    t = text.lower()\n",
    "    if any(k in t for k in [\"gesetz\", \"regierung\", \"bundestag\", \"politik\", \"eu\"]):\n",
    "        return \"Climate_Policy\"\n",
    "    if any(k in t for k in [\"studie\", \"forschung\", \"ipcc\"]):\n",
    "        return \"Climate_Science\"\n",
    "    if any(k in t for k in [\"energie\", \"wind\", \"solar\", \"strom\"]):\n",
    "        return \"Energy_Transition\"\n",
    "    if any(k in t for k in [\"wirtschaft\", \"industrie\", \"markt\"]):\n",
    "        return \"Climate_Economy\"\n",
    "    if any(k in t for k in [\"protest\", \"aktivisten\", \"demonstration\"]):\n",
    "        return \"Climate_Activism\"\n",
    "    if any(k in t for k in [\"hitzewelle\", \"dürre\", \"hochwasser\", \"waldbrand\"]):\n",
    "        return \"Climate_Impact\"\n",
    "    return \"Other_Climate\"\n",
    "rows = []\n",
    "seen_urls = set()\n",
    "for keyword in CLIMATE_KEYWORDS:\n",
    "    print(f\"\\nkeyword: {keyword}\")\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        print(f\"Page {page}\")\n",
    "        params = {\n",
    "            \"q\": keyword,\n",
    "            \"page\": page,\n",
    "            \"page_size\": PAGE_SIZE,\n",
    "            \"segments\": SEGMENTS,\n",
    "            \"after\": -2208988800,\n",
    "            \"before\": int(datetime.now().timestamp())}\n",
    "        r = requests.get(SEARCH_API, params=params, headers=HEADERS, timeout=15)\n",
    "        data = r.json()\n",
    "        items = data.get(\"results\", [])\n",
    "        if not items:\n",
    "            break\n",
    "        for item in items:\n",
    "            url = item.get(\"url\")\n",
    "            if not url or url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            title = clean_text(item.get(\"title\"))\n",
    "            intro = clean_text(item.get(\"teaser\") or item.get(\"intro\"))\n",
    "            html = requests.get(url, headers=HEADERS, timeout=15).text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            body = soup.find(\"div\", attrs={\"data-area\": \"body\"})\n",
    "            if not body:\n",
    "                continue\n",
    "            paragraphs = [\n",
    "                p.get_text().strip()\n",
    "                for p in body.find_all(\"p\")\n",
    "                if len(p.get_text().strip()) > 60]\n",
    "            content = clean_text(\" \".join(paragraphs))\n",
    "            if not content or len(content) < 1000:\n",
    "                continue\n",
    "            if is_boilerplate(content):\n",
    "                continue\n",
    "            relevance = keyword_hits(f\"{title} {intro} {content}\")\n",
    "            if relevance < 2:\n",
    "                continue\n",
    "            author = extract_author(soup)\n",
    "            pub_date = extract_published_date(soup)\n",
    "            is_premium = bool(soup.select_one(\"[data-area='Paywall']\"))\n",
    "            actors, actor_count, sent_count, length = analyze_text(content)\n",
    "            article_class = classify_article(content)\n",
    "            rows.append({\n",
    "                \"URL\": url,\n",
    "                \"Source\": SOURCE,\n",
    "                \"Language\": LANGUAGE,\n",
    "                \"Published_Date\": pub_date,\n",
    "                \"Keyword_Matched\": keyword,\n",
    "                \"Article_Classification\": article_class,\n",
    "                \"Headline\": title,\n",
    "                \"Intro\": intro,\n",
    "                \"Content\": content,\n",
    "                \"Content_Length\": length,\n",
    "                \"Sentence_Count\": sent_count,\n",
    "                \"Actors\": \", \".join(actors),\n",
    "                \"Actor_Count\": actor_count,\n",
    "                \"Author\": author\n",
    "            })\n",
    "            time.sleep(1)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"File saved: {OUTPUT_CSV}\")\n",
    "print(f\"Total number of climate articles saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6277c1-19b6-43be-be1d-0a96443810a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Sud deutsche scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef8e5f-1bfc-4ad2-b211-5eba771b606b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "CLIMATE_KEYWORDS = [\"klima\", \"klimakrise\", \"klimawandel\", \"erderwärmung\", \"globale erwärmung\", \"treibhauseffekt\", \"treibhausgas\",\n",
    "    \"co2\", \"kohlendioxid\", \"emission\", \"emissionen\", \"energiewende\", \"erneuerbare energien\", \"klimaschutz\", \"klimapolitik\",\n",
    "    \"hitzewelle\", \"dürre\", \"hochwasser\", \"wasserknappheit\", \"starkregen\", \"waldbrand\", \"gletscherschmelze\", \"artensterben\",\n",
    "    \"klimaneutral\", \"emissionshandel\"]\n",
    "STRICT_KEYWORDS = [\"klima\", \"klimakrise\", \"klimawandel\", \"erderwärmung\", \"co2\", \"kohlendioxid\", \"emission\", \"emissionen\",\n",
    "    \"energiewende\", \"klimaschutz\", \"klimaneutral\", \"treibhauseffekt\", \"treibhausgas\",\n",
    "    \"hitzewelle\", \"dürre\", \"hochwasser\", \"wasserknappheit\", \"starkregen\", \"waldbrand\"]\n",
    "CLASSIFICATION_KEYWORDS = {\n",
    "    \"Climate_Policy\": [\"politik\", \"regierung\", \"gesetz\", \"bundestag\", \"eu\"],\n",
    "    \"Climate_Science\": [\"studie\", \"forschung\", \"wissenschaft\", \"ipcc\"],\n",
    "    \"Energy_Transition\": [\"energiewende\", \"solar\", \"windkraft\", \"wasserstoff\"],\n",
    "    \"Climate_Economy\": [\"wirtschaft\", \"industrie\", \"kosten\", \"preis\"],\n",
    "    \"Climate_Impact\": [\"hitzewelle\", \"dürre\", \"hochwasser\", \"waldbrand\"],\n",
    "    \"Climate_Opinion\": [\"meinung\", \"kommentar\", \"kolumne\"]}\n",
    "SOURCE = \"SUEDDEUTSCHE_ZEITUNG\"\n",
    "LANGUAGE = \"de\"\n",
    "OUTPUT_CSV = \"scrapped_sd.csv\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "PAGES = range(1, 11)\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "def strict_relevance(title, content):\n",
    "    t = title.lower()\n",
    "    c = content.lower()\n",
    "    lead = \" \".join(c.split()[:300])\n",
    "    return any(k in t for k in STRICT_KEYWORDS) or sum(k in lead for k in STRICT_KEYWORDS) >= 3\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    actors = sorted(set(ent.text for ent in doc.ents if ent.label_ in [\"PER\", \"ORG\"]))\n",
    "    return actors, len(actors), len(list(doc.sents)), len(text)\n",
    "def classify_article(title, intro, content):\n",
    "    text = f\"{title} {intro or ''} {' '.join(content.split()[:300])}\".lower()\n",
    "    scores = {k: sum(w in text for w in v) for k, v in CLASSIFICATION_KEYWORDS.items()}\n",
    "    best = max(scores, key=scores.get)\n",
    "    return best if scores[best] > 0 else \"Other_Climate\"\n",
    "def extract_article(url):\n",
    "    html = requests.get(url, headers=HEADERS, timeout=15).text\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    headline = None\n",
    "    content = None\n",
    "    author = None\n",
    "    pub_date = None\n",
    "    for script in soup.find_all(\"script\", type=\"application/ld+json\"):\n",
    "        try:\n",
    "            data = json.loads(script.string)\n",
    "        except:\n",
    "            continue\n",
    "        if isinstance(data, dict) and data.get(\"@type\") == \"NewsArticle\":\n",
    "            headline = clean_text(data.get(\"headline\"))\n",
    "            content = clean_text(data.get(\"articleBody\"))\n",
    "            pub_date = data.get(\"datePublished\")\n",
    "            auth = data.get(\"author\")\n",
    "            if isinstance(auth, dict):\n",
    "                author = auth.get(\"name\")\n",
    "            elif isinstance(auth, list) and auth:\n",
    "                author = auth[0].get(\"name\")\n",
    "            break\n",
    "    if not content:\n",
    "        texts = [p.get_text(strip=True) for p in soup.find_all(\"p\") if len(p.get_text(strip=True)) > 80]\n",
    "        content = clean_text(\" \".join(texts))\n",
    "    if not headline:\n",
    "        h1 = soup.find(\"h1\")\n",
    "        headline = clean_text(h1.get_text()) if h1 else None\n",
    "    return headline, content, author, pub_date\n",
    "rows = []\n",
    "for keyword in CLIMATE_KEYWORDS:\n",
    "    print(f\"\\nKeyword: {keyword}\")\n",
    "    for page in PAGES:\n",
    "        r = requests.get(\"https://www.sueddeutsche.de/suche\", params={\"q\": keyword, \"page\": page}, headers=HEADERS, timeout=15)\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        links = soup.select(\"a[href^='https://www.sueddeutsche.de']\")\n",
    "        if not links:\n",
    "            break\n",
    "        for a in links:\n",
    "            url = a.get(\"href\")\n",
    "            if not url:\n",
    "                continue\n",
    "            try:\n",
    "                headline, content, author, pub_date = extract_article(url)\n",
    "            except:\n",
    "                continue\n",
    "            if not headline or not content:\n",
    "                continue\n",
    "            if not strict_relevance(headline, content):\n",
    "                continue\n",
    "            actors, actor_count, sent_count, length = analyze_text(content)\n",
    "            article_class = classify_article(headline, None, content)\n",
    "            rows.append({\n",
    "                \"URL\": url,\n",
    "                \"Source\": SOURCE,\n",
    "                \"Language\": LANGUAGE,\n",
    "                \"Published_Date\": pub_date,\n",
    "                \"Keyword_Matched\": keyword,\n",
    "                \"Article_Classification\": article_class,\n",
    "                \"Headline\": headline,\n",
    "                \"Intro\": None,\n",
    "                \"Content\": content,\n",
    "                \"Content_Length\": length,\n",
    "                \"Sentence_Count\": sent_count,\n",
    "                \"Actors\": \", \".join(actors),\n",
    "                \"Actor_Count\": actor_count,\n",
    "                \"Author\": author\n",
    "            })\n",
    "            print(\"Saved\")\n",
    "            time.sleep(0.6)\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"File saved: {OUTPUT_CSV}\")\n",
    "print(f\"Total number of climate articles saved: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00172da-9fee-4ba7-973e-9e87ddec3883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for scrapping BZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab579cbd-6877-4f3d-9f07-1af66c19acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "BASE_SEARCH = \"https://www.bz-berlin.de/suche\"\n",
    "BASE_KEYWORDS = [\"Klima\", \"Klimawandel\", \"Klimakrise\", \"CO2\", \"Emissionen\", \"Energiewende\", \"Hochwasser\", \"Dürre\", \"Hitzewelle\", \"Klimaprotest\",\n",
    "    \"Klimapolitik\"]\n",
    "INVALID_URL_PATTERNS = [\"/video/\", \"/bilder/\", \"/spiele/\", \"/angebote/\", \"/shopping/\"]\n",
    "CLIMATE_KEYWORDS_DE = [\"klima\", \"klimawandel\", \"klimakrise\", \"erderwärmung\", \"co2\", \"emission\", \"energiewende\", \"hochwasser\",\n",
    "    \"dürre\", \"hitzewelle\", \"treibhaus\", \"klimapolitik\", \"klimaschutz\", \"klimaaktiv\", \"klimaprotest\", \"letzte generation\"]\n",
    "YEARS = range(2022, 2027)\n",
    "OUTPUT_FILE = \"scrapped_bz.csv\"\n",
    "def extract_actors(text):\n",
    "    doc = nlp(text[:4000])\n",
    "    return sorted(set(\n",
    "        ent.text for ent in doc.ents\n",
    "        if ent.label_ in (\"PER\", \"ORG\")))\n",
    "def sentence_count(text):\n",
    "    return sum(1 for _ in nlp(text).sents)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "chrome_options.add_argument(r\"--user-data-dir=C:/selenium_chrome_profile\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "def human_sleep(a=3, b=6):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "def safe_get(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        human_sleep(2, 4)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "def extract_date():\n",
    "    try:\n",
    "        return driver.find_element(By.CSS_SELECTOR, \"div.entry-meta small\").text.strip()\n",
    "    except:\n",
    "        return None\n",
    "def extract_author():\n",
    "    try:\n",
    "        author = driver.find_element(By.CSS_SELECTOR, \"a[rel='author']\")\n",
    "        return author.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        container = driver.find_element(By.CSS_SELECTOR, 'div[data-type=\"author\"]')\n",
    "        span = container.find_element(By.TAG_NAME, \"span\")\n",
    "        return span.text.strip()\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "def extract_article():\n",
    "    paragraphs = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "    clean = []\n",
    "    for p in paragraphs:\n",
    "        t = p.text.strip()\n",
    "        if len(t) < 60:\n",
    "            continue\n",
    "        if any(x in t.lower() for x in [\"anzeige\", \"newsletter\", \"abonnieren\", \"quelle:\", \"bild:\"]):\n",
    "            continue\n",
    "        clean.append(t)\n",
    "    if len(clean) < 5:\n",
    "        return None, None\n",
    "    intro = clean[0]\n",
    "    content = re.sub(r\"\\s+\", \" \", \" \".join(clean))\n",
    "    return intro, content\n",
    "def is_climate_article(title, content):\n",
    "    title = title.lower()\n",
    "    content = content.lower()\n",
    "    if any(x in title for x in [\"horoskop\", \"quiz\", \"promi\"]):\n",
    "        return False\n",
    "    keyword_hits = sum(content.count(k) for k in CLIMATE_KEYWORDS_DE)\n",
    "    strong_signals = [\"klimapolitik\", \"klimaschutz\", \"klimaaktiv\", \"klimaprotest\", \"energiewende\", \"co2-preis\", \"letzte generation\", \"klima-demo\"]\n",
    "    strong_hit = any(s in content for s in strong_signals)\n",
    "    return (keyword_hits >= 3 or strong_hit or any(k in title for k in CLIMATE_KEYWORDS_DE))\n",
    "def derive_climate_section(title, content):\n",
    "    text = f\"{title} {content}\".lower()\n",
    "    if any(k in text for k in [\"gesetz\", \"regierung\", \"bundestag\", \"verordnung\", \"klimapolitik\", \"ziel\"]):\n",
    "        return \"Climate_Policy\"\n",
    "    if any(k in text for k in [\"studie\", \"forschung\", \"wissenschaftler\", \"ipcc\", \"daten\", \"bericht\"]):\n",
    "        return \"Climate_Science\"\n",
    "    if any(k in text for k in [\"energiewende\", \"erneuerbar\", \"solar\", \"wind\", \"kohlekraft\", \"gas\", \"strom\"]):\n",
    "        return \"Energy_Transition\"\n",
    "    if any(k in text for k in [\"kosten\", \"preise\", \"wirtschaft\", \"industrie\", \"markt\", \"subvention\"]):\n",
    "        return \"Climate_Economy\"\n",
    "    if any(k in text for k in [\"klimaprotest\", \"demonstration\", \"aktivisten\", \"letzte generation\", \"blockade\", \"ngo\"]):\n",
    "        return \"Climate_Activism\"\n",
    "    if any(k in text for k in [\"hochwasser\", \"überschwemmung\", \"dürre\", \"hitzewelle\", \"katastrophe\"]):\n",
    "        return \"Climate_Impact\"\n",
    "    if any(k in text for k in [\"eu\", \"china\", \"usa\",\"international\", \"global\"]):\n",
    "        return \"Climate_Geopolitics\"\n",
    "    if any(k in text for k in [\"meinung\", \"kommentar\", \"kolumne\", \"leitartikel\"]):\n",
    "        return \"Climate_Opinion\"\n",
    "    return \"Other_Climate\"\n",
    "rows = []\n",
    "visited = set()\n",
    "for base_keyword in BASE_KEYWORDS:\n",
    "    for year in YEARS:\n",
    "        search_term = f\"{base_keyword} {year}\"\n",
    "        search_url = f\"{BASE_SEARCH}?q={search_term}\"\n",
    "        if not safe_get(search_url):\n",
    "            continue\n",
    "        articles = driver.find_elements(By.CSS_SELECTOR, \"article a[href]\")\n",
    "        urls = []\n",
    "        for a in articles:\n",
    "            url = a.get_attribute(\"href\")\n",
    "            if not url:\n",
    "                continue\n",
    "            if not url.startswith(\"https://www.bz-berlin.de\"):\n",
    "                continue\n",
    "            if any(bad in url for bad in INVALID_URL_PATTERNS):\n",
    "                continue\n",
    "            if url in visited:\n",
    "                continue\n",
    "            visited.add(url)\n",
    "            urls.append(url)\n",
    "        for url in urls:\n",
    "            if not safe_get(url):\n",
    "                continue\n",
    "            try:\n",
    "                wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "                headline = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "                intro, content = extract_article()\n",
    "                if not content:\n",
    "                    continue\n",
    "                if not is_climate_article(headline, content):\n",
    "                    continue\n",
    "                actors = extract_actors(content)\n",
    "                derived_section = derive_climate_section(headline, content)\n",
    "                rows.append({\n",
    "                    \"URL\": url,\n",
    "                    \"Source\": \"B.Z.\",\n",
    "                    \"Language\": \"de\",\n",
    "                    \"Published_Date\": extract_date(),\n",
    "                    \"Article_Classification\": derived_section,\n",
    "                    \"Keyword_Matched\": search_term,\n",
    "                    \"Headline\": headline,\n",
    "                    \"Intro\": intro,\n",
    "                    \"Content\": content,\n",
    "                    \"Content_Length\": len(content),\n",
    "                    \"Sentence_Count\": sentence_count(content),\n",
    "                    \"Actors\": \", \".join(actors),\n",
    "                    \"Actor_Count\": len(actors),\n",
    "                    \"Author\": extract_author()\n",
    "                })\n",
    "                print(\"Saved:\", headline[:60], \"| Author:\", extract_author())\n",
    "                human_sleep()\n",
    "            except:\n",
    "                continue\n",
    "df = pd.DataFrame(rows)\n",
    "df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "driver.quit()\n",
    "print(f\"File: {OUTPUT_FILE}\")\n",
    "print(f\"Total number of Articles scrapped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b597d7a0-2767-4ab7-b2c1-68c118e6d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for scrapping die ziet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f92472b-e393-451a-bcdf-2e517ba0a8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import spacy\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "BASE_KEYWORDS = [\"Klima\", \"Klimawandel\", \"Klimakrise\", \"CO2\", \"Emissionen\", \"Energiewende\", \"Hochwasser\", \"Dürre\", \"Hitzewelle\",\n",
    "    \"Klimaprotest\", \"Klimapolitik\"]\n",
    "INVALID_URL_PATTERNS = [ \"/video/\", \"/angebote/\", \"/spiele/\", \"/campus/\", \"/index\"]\n",
    "CLIMATE_KEYWORDS_DE = [\"klima\", \"klimawandel\", \"klimakrise\", \"erderwärmung\", \"co2\", \"emission\", \"energiewende\", \"hochwasser\",\n",
    "    \"dürre\", \"hitzewelle\", \"treibhaus\", \"klimapolitik\", \"klimaschutz\", \"klimaaktiv\", \"klimaprotest\", \"letzte generation\"]\n",
    "STRONG_CLIMATE_TERMS = [\"klimawandel\", \"klimakrise\", \"erderwärmung\", \"klimaschutz\", \"klimapolitik\", \"klimaprotest\", \"klimaaktivisten\",\n",
    "    \"letzte generation\", \"co2-preis\", \"emissionshandel\", \"klimaziele\", \"pariser abkommen\", \"ipcc\"]\n",
    "BASE_SEARCH = \"https://www.zeit.de/suche/index\"\n",
    "YEARS = range(2022, 2026)\n",
    "PAGES = range(1, 99)\n",
    "OUTPUT_FILE = \"scrapped_zeit.csv\"\n",
    "def extract_actors(text):\n",
    "    doc = nlp(text[:4000])\n",
    "    return sorted(set(\n",
    "        ent.text for ent in doc.ents\n",
    "        if ent.label_ in (\"PER\", \"ORG\")))\n",
    "def sentence_count(text):\n",
    "    return sum(1 for _ in nlp(text).sents)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)\")\n",
    "chrome_options.add_argument(r\"--user-data-dir=C:/selenium_chrome_profile\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "def human_sleep(a=2, b=4):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "def safe_get(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        human_sleep()\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "def extract_date():\n",
    "    try:\n",
    "        return driver.find_element(By.TAG_NAME, \"time\").get_attribute(\"datetime\")\n",
    "    except:\n",
    "        return None\n",
    "def extract_author():\n",
    "    try:\n",
    "        authors = driver.find_elements(\n",
    "            By.CSS_SELECTOR,\n",
    "            'a[rel=\"author\"] span[itemprop=\"name\"]'\n",
    "        )\n",
    "        return \", \".join(a.text.strip() for a in authors)\n",
    "    except:\n",
    "        return None\n",
    "def extract_article():\n",
    "    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"article p\")\n",
    "    clean = []\n",
    "    for p in paragraphs:\n",
    "        text = p.text.strip()\n",
    "        if len(text) < 70:\n",
    "            continue\n",
    "        if any(x in text.lower() for x in [\"anzeige\", \"newsletter\", \"abonnieren\"]):\n",
    "            continue\n",
    "        clean.append(text)\n",
    "    if len(clean) < 6:\n",
    "        return None, None\n",
    "    intro = clean[0]\n",
    "    content = re.sub(r\"\\s+\", \" \", \" \".join(clean))\n",
    "    return intro, content\n",
    "def is_climate_article(title, content):\n",
    "    title_l = title.lower()\n",
    "    content_l = content.lower()\n",
    "    if any(x in title_l for x in [\"newsletter\", \"horoskop\", \"quiz\", \"sport\", \"kultur\", \"reise\"]):\n",
    "        return False\n",
    "    keyword_hits = sum(content_l.count(k) for k in CLIMATE_KEYWORDS_DE)\n",
    "    strong_hits = sum(1 for k in STRONG_CLIMATE_TERMS if k in content_l or k in title_l)\n",
    "    density = keyword_hits / max(len(content_l), 1)\n",
    "    return (strong_hits >= 2 or (strong_hits >= 1 and keyword_hits >= 10) or (\"klima\" in title_l and keyword_hits >= 8 and density >= 0.001))\n",
    "def derive_climate_section(title, content):\n",
    "    text = f\"{title} {content}\".lower()\n",
    "    if any(k in text for k in [\"gesetz\", \"regierung\", \"bundestag\"]):\n",
    "        return \"Climate_Policy\"\n",
    "    if any(k in text for k in [\"studie\", \"ipcc\", \"forschung\"]):\n",
    "        return \"Climate_Science\"\n",
    "    if any(k in text for k in [\"energie\", \"wind\", \"solar\", \"strom\"]):\n",
    "        return \"Energy_Transition\"\n",
    "    if any(k in text for k in [\"wirtschaft\", \"preise\", \"markt\"]):\n",
    "        return \"Climate_Economy\"\n",
    "    if any(k in text for k in [\"protest\", \"aktivisten\"]):\n",
    "        return \"Climate_Activism\"\n",
    "    if any(k in text for k in [\"hochwasser\", \"dürre\", \"hitzewelle\"]):\n",
    "        return \"Climate_Impact\"\n",
    "    if any(k in text for k in [\"eu\", \"usa\", \"china\"]):\n",
    "        return \"Climate_Geopolitics\"\n",
    "    if any(k in text for k in [\"meinung\", \"kommentar\"]):\n",
    "        return \"Climate_Opinion\"\n",
    "    return \"Other_Climate\"\n",
    "rows = []\n",
    "visited_urls = set()\n",
    "visited_hashes = set()\n",
    "for keyword in BASE_KEYWORDS:\n",
    "    for year in YEARS:\n",
    "        query = f\"{keyword} {year}\"\n",
    "        for page in PAGES:\n",
    "            search_url = f\"{BASE_SEARCH}?p={page}&q={query}\"\n",
    "            print(f\"{query} | Page {page}\")\n",
    "            if not safe_get(search_url):\n",
    "                continue\n",
    "            links = driver.find_elements(By.CSS_SELECTOR, \"article a[href]\")\n",
    "            urls = []\n",
    "            for a in links:\n",
    "                try:\n",
    "                    href = a.get_attribute(\"href\")\n",
    "                    if not href:\n",
    "                        continue\n",
    "                    if not href.startswith(\"https://www.zeit.de/\"):\n",
    "                        continue\n",
    "                    if any(bad in href for bad in INVALID_URL_PATTERNS):\n",
    "                        continue\n",
    "                    if href in visited_urls:\n",
    "                        continue\n",
    "                    visited_urls.add(href)\n",
    "                    urls.append(href)\n",
    "                except:\n",
    "                    continue\n",
    "            for url in urls:\n",
    "                if not safe_get(url):\n",
    "                    continue\n",
    "                try:\n",
    "                    wait.until(EC.presence_of_element_located((By.TAG_NAME, \"h1\")))\n",
    "                    headline = driver.find_element(By.TAG_NAME, \"h1\").text.strip()\n",
    "                    intro, content = extract_article()\n",
    "                    if not content:\n",
    "                        continue\n",
    "                    if not is_climate_article(headline, content):\n",
    "                        continue\n",
    "                    content_hash = hashlib.md5(\n",
    "                        content[:1000].encode(\"utf-8\")\n",
    "                    ).hexdigest()\n",
    "                    if content_hash in visited_hashes:\n",
    "                        continue\n",
    "                    visited_hashes.add(content_hash)\n",
    "                    actors = extract_actors(content)\n",
    "                    rows.append({\n",
    "                        \"URL\": url,\n",
    "                        \"Source\": \"DIE ZEIT\",\n",
    "                        \"Language\": \"de\",\n",
    "                        \"Published_Date\": extract_date(),\n",
    "                        \"Derived_Section\": derive_climate_section(headline, content),\n",
    "                        \"Keyword_Matched\": query,\n",
    "                        \"Headline\": headline,\n",
    "                        \"Intro\": intro,\n",
    "                        \"Content\": content,\n",
    "                        \"Content_Length\": len(content),\n",
    "                        \"Sentence_Count\": sentence_count(content),\n",
    "                        \"Actors\": \", \".join(actors),\n",
    "                        \"Actor_Count\": len(actors),\n",
    "                        \"Author\": extract_author()\n",
    "                    })\n",
    "                    print(\"Saved:\", headline[:70])\n",
    "                    human_sleep()\n",
    "                except:\n",
    "                    continue\n",
    "df = pd.DataFrame(rows)\n",
    "df.drop_duplicates(subset=[\"URL\"], inplace=True)\n",
    "df.to_csv(OUTPUT_FILE, index=False, encoding=\"utf-8-sig\")\n",
    "driver.quit()\n",
    "print(f\"Saved File: {OUTPUT_FILE}\")\n",
    "print(f\"Total number of articles scrapped: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d6ac7-72f6-4985-bbe7-1a0f8fcb123f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
